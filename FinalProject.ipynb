{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6617d25d-727a-424a-a035-967ca45657c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6031eff8-d17e-4c43-9545-48c897c41ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['category', 'rating', 'label', 'text_'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('C:/Users/JOE/Desktop/Final Project/fake reviews dataset.csv')\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb8f8b56-1c90-428c-8b84-b2381fa50fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\JOE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JOE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JOE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               text_  \\\n",
      "0  Love this!  Well made, sturdy, and very comfor...   \n",
      "1  love it, a great upgrade from the original.  I...   \n",
      "2  This pillow saved my back. I love the look and...   \n",
      "3  Missing information on how to use it, but it i...   \n",
      "4  Very nice set. Good quality. We have had the s...   \n",
      "\n",
      "                                    cleaned_text  \n",
      "0  love well made sturdy comfortable love pretty  \n",
      "1   love great upgrade original mine couple year  \n",
      "2        pillow saved back love look feel pillow  \n",
      "3    missing information use great product price  \n",
      "4            nice set good quality set two month  \n"
     ]
    }
   ],
   "source": [
    "# --- Text Preprocessing ---\n",
    "# Download necessary NLTK packages\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the input text by tokenizing, removing stopwords, and lemmatizing.\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
    "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(cleaned_words)\n",
    "\n",
    "# Apply the cleaning function to the 'text_' column (adjust column name as needed)\n",
    "data['cleaned_text'] = data['text_'].apply(clean_text)\n",
    "\n",
    "# Preview the cleaned data\n",
    "print(data[['text_', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869f6b46-27f5-443d-8267-44d07b4b1a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8430814888092\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.82      0.88      0.85      4016\n",
      "          OR       0.87      0.81      0.84      4071\n",
      "\n",
      "    accuracy                           0.84      8087\n",
      "   macro avg       0.84      0.84      0.84      8087\n",
      "weighted avg       0.84      0.84      0.84      8087\n",
      "\n",
      "Model and vectorizer saved.\n"
     ]
    }
   ],
   "source": [
    "# --- Classification (Traditional Machine Learning) ---\n",
    "# Split data into features (X) and target labels (y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['cleaned_text'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_vect, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_vect)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model and vectorizer\n",
    "joblib.dump(model, 'text_classifier_model.pkl')\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "print(\"Model and vectorizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6e8666-1819-44a1-80a6-f8a7a4e9cb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 65ms/step - accuracy: 0.7981 - loss: 0.4243 - val_accuracy: 0.8907 - val_loss: 0.2630\n",
      "Epoch 2/5\n",
      "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 64ms/step - accuracy: 0.9160 - loss: 0.2045 - val_accuracy: 0.9084 - val_loss: 0.2305\n",
      "Epoch 3/5\n",
      "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 62ms/step - accuracy: 0.9407 - loss: 0.1494 - val_accuracy: 0.9085 - val_loss: 0.2469\n",
      "Epoch 4/5\n",
      "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 64ms/step - accuracy: 0.9594 - loss: 0.1069 - val_accuracy: 0.9094 - val_loss: 0.2555\n",
      "Epoch 5/5\n",
      "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 61ms/step - accuracy: 0.9689 - loss: 0.0846 - val_accuracy: 0.9044 - val_loss: 0.2781\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step\n",
      "Accuracy (LSTM): 0.9044144923952022\n",
      "Classification Report (LSTM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90      4016\n",
      "           1       0.90      0.91      0.91      4071\n",
      "\n",
      "    accuracy                           0.90      8087\n",
      "   macro avg       0.90      0.90      0.90      8087\n",
      "weighted avg       0.90      0.90      0.90      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Classification (Deep Learning with LSTM) ---\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare data for LSTM\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "max_sequence_length = 100\n",
    "X_train_seq_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
    "X_test_seq_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "# Build the LSTM model\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128),  # Removed input_length\n",
    "    LSTM(128),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train_seq_pad, y_train, validation_data=(X_test_seq_pad, y_test), epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the LSTM model\n",
    "y_pred_lstm = (model_lstm.predict(X_test_seq_pad) > 0.5).astype(\"int32\")\n",
    "print(\"Accuracy (LSTM):\", accuracy_score(y_test, y_pred_lstm))\n",
    "print(\"Classification Report (LSTM):\")\n",
    "print(classification_report(y_test, y_pred_lstm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "750fd74d-bfd4-4564-8237-a448c07cef6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "price, good, speaker, get, product, use, great, one, sound, work\n",
      "Topic #2:\n",
      "good, work, small, little, nice, light, size, comfortable, great, fit\n",
      "Topic #3:\n",
      "son, little, product, cat, toy, one, bought, great, dog, love\n",
      "Topic #4:\n",
      "one, blade, show, saw, great, watch, acting, film, good, movie\n",
      "Topic #5:\n",
      "reading, enjoyed, author, good, well, series, character, story, read, book\n"
     ]
    }
   ],
   "source": [
    "# --- Topic Modeling (Using LDA) ---\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Transform text with TF-IDF\n",
    "X_tfidf = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Apply LDA for topic modeling\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X_tfidf)\n",
    "\n",
    "# Display the top words in each topic\n",
    "n_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\", \".join([feature_names[i] for i in topic.argsort()[-n_top_words:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c80a3-881f-4305-ad3a-2dbe6b322b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d809894-da52-45e3-9328-2e8bd755522d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8c00c-5f84-4a4e-a64c-5a368dd3f79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872024f-1d3b-4661-ac81-2f769226704b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f33893-9df6-45b4-9f94-2e87b5c08360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cec897-bb68-4e2c-be37-d7a7f96861f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003956f6-f86f-403a-8df4-9e0c7757fbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
